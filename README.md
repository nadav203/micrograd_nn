# micrograd_nn

This repository contains my personal take on Andrej Karpathy’s Micrograd project. Although Micrograd is small in size, it reveals the backbone of how deep learning models are structured and trained. It supplies an automatic differentiation mechanism (autograd) and can be employed to build and refine neural networks by adjusting their parameters.

I plan to start by recreating the foundational code. Afterward, I’ll look for opportunities to add new features or improve existing functionality to make it even more versatile.

References:

[Andrej's Micrograd Repository](https://github.com/karpathy/micrograd)

[Andrej's Micrograd Playlist](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&si=FeJvCmvhqMjxbAAs)